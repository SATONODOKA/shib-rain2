import os
import glob
import chromadb
import streamlit as st
from sentence_transformers import SentenceTransformer
import requests
import json
from typing import List, Dict, Any
import re


# è¨­å®š
LM_STUDIO_API_URL = "http://localhost:1234/v1/chat/completions"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

class RAGSystem:
    def __init__(self):
        self.embedding_status = ""
        self.db_status = ""
        self.setup_embedding_model()
        self.setup_chroma_db()
        self.auto_load_documents()
        
    def setup_embedding_model(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        try:
            # æ—¥æœ¬èªå¯¾å¿œã®è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            self.embedding_model = SentenceTransformer('intfloat/multilingual-e5-small')
            self.embedding_status = "âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ"
        except Exception as e:
            self.embedding_status = f"âŒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}"
            return False
        return True
    
    def setup_chroma_db(self):
        """ChromaDBã®åˆæœŸåŒ–"""
        try:
            # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
            self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆã¾ãŸã¯å–å¾—
            try:
                self.collection = self.chroma_client.get_collection("sales_knowledge")
                self.db_status = f"ğŸ“š æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{self.collection.count()}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰"
            except:
                self.collection = self.chroma_client.create_collection(
                    name="sales_knowledge",
                    metadata={"description": "å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"}
                )
                self.db_status = "ğŸ“š æ–°ã—ã„ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ"
                
        except Exception as e:
            self.db_status = f"âŒ ChromaDBã®åˆæœŸåŒ–ã«å¤±æ•—: {e}"
            return False
        return True
    
    def auto_load_documents(self):
        """åˆæœŸåŒ–æ™‚ã«è‡ªå‹•ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            current_count = self.collection.count()
            if current_count > 0:
                self.db_status = f"âœ… {current_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™"
                return True
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç©ºã®å ´åˆã¯è‡ªå‹•èª­ã¿è¾¼ã¿
            document_path = "sample_documents"
            if os.path.exists(document_path):
                self.load_documents_silent(document_path)
            else:
                self.db_status = "âš ï¸ sample_documentsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                
        except Exception as e:
            self.db_status = f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
    
    def load_documents_silent(self, document_path: str = "sample_documents"):
        """UIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆæœŸåŒ–ç”¨ï¼‰"""
        if not os.path.exists(document_path):
            self.db_status = f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}"
            return False
        
        # æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        try:
            self.chroma_client.delete_collection("sales_knowledge")
            self.collection = self.chroma_client.create_collection(
                name="sales_knowledge",
                metadata={"description": "å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"}
            )
        except:
            pass
            
        # å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        file_patterns = [
            "*.md", "*.txt", "*.docx.md"  # ä»Šå›ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        ]
        
        documents = []
        for pattern in file_patterns:
            files = glob.glob(os.path.join(document_path, pattern))
            documents.extend(files)
        
        documents.sort()  # å®‰å®šã—ãŸå‡¦ç†é †åºã®ãŸã‚
        
        if not documents:
            self.db_status = "âŒ èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            return False
        
        total_chunks = 0
        for doc_path in documents:
            chunks_added = self.process_document(doc_path)
            total_chunks += chunks_added
        
        self.db_status = f"âœ… {len(documents)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{total_chunks}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"
        return True
    
    def load_documents(self, document_path: str = "sample_documents"):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        if not os.path.exists(document_path):
            st.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {document_path}")
            return False
        
        # æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        try:
            self.chroma_client.delete_collection("sales_knowledge")
            self.collection = self.chroma_client.create_collection(
                name="sales_knowledge",
                metadata={"description": "å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"}
            )
            st.info("ğŸ”„ æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except:
            pass
            
        # å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        file_patterns = [
            "*.md", "*.txt", "*.docx.md"  # ä»Šå›ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
        ]
        
        documents = []
        for pattern in file_patterns:
            files = glob.glob(os.path.join(document_path, pattern))
            documents.extend(files)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åå‰ã§ã‚½ãƒ¼ãƒˆï¼ˆå‡¦ç†é †åºã‚’å®‰å®šåŒ–ï¼‰
        documents.sort()
        
        if not documents:
            st.warning("âš ï¸ èª­ã¿è¾¼ã¿å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
            
        st.info(f"ğŸ“„ {len(documents)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†
        processed_count = 0
        total_chunks = 0
        progress_bar = st.progress(0)
        
        for i, file_path in enumerate(documents):
            try:
                st.text(f"ğŸ“– å‡¦ç†ä¸­: {os.path.basename(file_path)}")
                chunks_added = self.process_document(file_path)
                if chunks_added > 0:
                    processed_count += 1
                    total_chunks += chunks_added
                    st.text(f"   âœ… {chunks_added}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                else:
                    st.text(f"   âš ï¸ å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                progress_bar.progress((i + 1) / len(documents))
            except Exception as e:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({os.path.basename(file_path)}): {e}")
        
        st.success(f"âœ… {processed_count}/{len(documents)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã—ãŸ")
        st.info(f"ğŸ“Š åˆè¨ˆ {total_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        return processed_count > 0
    
    def process_document(self, file_path: str) -> int:
        """å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                return 0
                
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å‡ºå…¸æƒ…å ±ã‚’å–å¾—
            filename = os.path.basename(file_path)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self.split_text(content, CHUNK_SIZE, CHUNK_OVERLAP)
            
            if not chunks:
                return 0
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦DBã«ä¿å­˜
            chunks_added = 0
            for i, chunk in enumerate(chunks):
                if not chunk.strip():
                    continue
                    
                chunk_id = f"{filename}#chunk-{i+1}"
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                try:
                    existing = self.collection.get(ids=[chunk_id])
                    if existing['ids']:
                        continue  # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                except:
                    pass
                
                # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—
                embedding = self.embedding_model.encode(chunk).tolist()
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                metadata = {
                    "source": filename,
                    "chunk_id": chunk_id,
                    "chunk_index": i,
                    "file_path": file_path
                }
                
                # ChromaDBã«ä¿å­˜
                self.collection.add(
                    documents=[chunk],
                    embeddings=[embedding],
                    metadatas=[metadata],
                    ids=[chunk_id]
                )
                chunks_added += 1
            
            return chunks_added
            
        except Exception as e:
            st.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def split_text(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡åŒºåˆ‡ã‚Šã§ã®åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹å ´åˆã¯æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk)
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®
                overlap_text = current_chunk[-chunk_overlap:] if len(current_chunk) > chunk_overlap else current_chunk
                current_chunk = overlap_text + sentence
            else:
                current_chunk += sentence + "ã€‚"
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def search_similar_documents(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢"""
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if self.collection.count() == 0:
                st.warning("âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™ã€‚ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                return []
            
            # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # ChromaDBã§é¡ä¼¼æ¤œç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results, self.collection.count()),
                include=["documents", "metadatas", "distances"]
            )
            
            # çµæœãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not results["documents"] or not results["documents"][0]:
                st.warning("âš ï¸ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return []
            
            # çµæœã®æ•´å½¢
            search_results = []
            for i in range(len(results["documents"][0])):
                search_results.append({
                    "content": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i],
                    "source": results["metadatas"][0][i]["source"]
                })
            
            return search_results
            
        except Exception as e:
            st.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ è§£æ±ºç­–: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """LM Studioã‚’ä½¿ç”¨ã—ã¦å›ç­”ç”Ÿæˆ"""
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
            context = "\n\n".join([
                f"ã€å‡ºå…¸: {doc['source']}ã€‘\n{doc['content']}"
                for doc in context_docs
            ])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            prompt = f"""ã‚ãªãŸã¯æ³•äººå‘ã‘ç ”ä¿®äº‹æ¥­ã®å–¶æ¥­æ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

# è³ªå•
{query}

# å‚è€ƒæƒ…å ±
{context}

# å›ç­”å½¢å¼
ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## ã€çµè«–ã€‘
è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºã§ç°¡æ½”ãªç­”ãˆ

## ã€æ ¹æ‹ ãƒ»è©³ç´°ã€‘
å‚è€ƒæƒ…å ±ã‹ã‚‰æŠœç²‹ã—ãŸå…·ä½“çš„ãªæ ¹æ‹ ã‚„è©³ç´°èª¬æ˜

## ã€å‡ºå…¸ã€‘
å‚è€ƒã«ã—ãŸæ–‡æ›¸åï¼ˆå½¢å¼ï¼šãƒ•ã‚¡ã‚¤ãƒ«å#ãƒãƒ£ãƒ³ã‚¯ç•ªå·ï¼‰

å›ç­”ã¯æ—¥æœ¬èªã§ã€å–¶æ¥­æ‹…å½“è€…ãŒé¡§å®¢ã«èª¬æ˜ã™ã‚‹éš›ã«ä½¿ãˆã‚‹å®Ÿç”¨çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚"""

            # LM Studio APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            headers = {
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "gpt-oss-20b",  # LM Studioã§èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«å
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            response = requests.post(LM_STUDIO_API_URL, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"âŒ LM Studio APIã‚¨ãƒ©ãƒ¼: {response.status_code}\n{response.text}"
                
        except requests.exceptions.ConnectionError:
            return "âŒ LM Studioã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚LM StudioãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        except requests.exceptions.Timeout:
            return "âŒ LM Studioã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚"
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def query(self, question: str) -> tuple[str, List[Dict[str, Any]]]:
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        # 1. é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢
        search_results = self.search_similar_documents(question, n_results=3)
        
        if not search_results:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", []
        
        # 2. LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆï¼ˆLM Studioæ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼‰
        try:
            response = requests.get("http://localhost:1234/v1/models", timeout=2)
            if response.status_code == 200:
                answer = self.generate_answer(question, search_results)
            else:
                answer = self.generate_fallback_answer(question, search_results)
        except:
            answer = self.generate_fallback_answer(question, search_results)
        
        return answer, search_results
    
    def generate_fallback_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """LM Studioæœªæ¥ç¶šæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”ç”Ÿæˆ"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ åŒ–å›ç­”ã‚’ç”Ÿæˆ
        answer = f"""## ã€æ¤œç´¢çµæœã€‘
è³ªå•: {query}

## ã€é–¢é€£æƒ…å ±ã€‘
"""
        
        for i, doc in enumerate(context_docs, 1):
            similarity = 1 - doc['distance']
            answer += f"""
### {i}. {doc['source']} (é¡ä¼¼åº¦: {similarity:.3f})
{doc['content'][:300]}{"..." if len(doc['content']) > 300 else ""}

"""
        
        answer += """
## ã€å‡ºå…¸ã€‘
"""
        for i, doc in enumerate(context_docs, 1):
            answer += f"- {doc['metadata']['chunk_id']}\n"
        
        answer += """
â„¹ï¸ **ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã‚’å¾—ã‚‹ã«ã¯**: LM Studioã‚’èµ·å‹•ã—ã¦gpt-oss-20bãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚"""
        
        return answer


def main():
    st.set_page_config(
        page_title="å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ RAGã‚·ã‚¹ãƒ†ãƒ ",
        page_icon="ğŸ’¼",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ã‚«ã‚¹ã‚¿ãƒ CSS - ChatGPTé¢¨ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
    st.markdown("""
    <style>
    /* ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š */
    .main {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }
    
    /* ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .main .block-container {
        padding-top: 0.5rem;
        padding-bottom: 2rem;
        max-width: 50rem;
        margin: 0 auto;
    }
    
    /* ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° - å·¦å¯„ã›ã§ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆ */
    .main-title {
        text-align: left;
        color: #1f2937;
        font-size: 1.5rem;
        font-weight: 700;
        margin-bottom: 0.25rem;
        letter-spacing: -0.025em;
    }
    
    .main-subtitle {
        text-align: left;
        color: #6b7280;
        font-size: 0.8rem;
        margin-bottom: 0.5rem;
        font-weight: 400;
    }
    
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ã‚³ãƒ³ãƒ†ãƒŠã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .header-container {
        margin-bottom: 1rem;
        padding-bottom: 1rem;
        border-bottom: 1px solid #e5e7eb;
    }
    
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stTextArea textarea {
        border: 1px solid #d1d5db;
        border-radius: 0.75rem;
        padding: 1rem;
        font-size: 0.95rem;
        line-height: 1.5;
        background-color: #ffffff;
        color: #111827 !important;
        caret-color: #111827 !important; /* Cursor color */
        resize: none;
        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        transition: all 0.2s ease;
    }
    
    .stTextArea textarea:focus {
        outline: none;
        border-color: #10b981;
        box-shadow: 0 0 0 3px rgba(16, 185, 129, 0.1);
        color: #111827 !important;
    }
    
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ */
    .stTextArea textarea::placeholder {
        color: #9ca3af !important;
        opacity: 1;
    }
    
    /* ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stButton > button {
        background-color: #10b981;
        color: white;
        border: none;
        border-radius: 0.5rem;
        padding: 0.625rem 1.5rem;
        font-weight: 600;
        font-size: 0.875rem;
        transition: all 0.2s ease;
        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    }
    
    .stButton > button:hover {
        background-color: #059669;
        transform: translateY(-1px);
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    
    .stButton > button:disabled {
        background-color: #d1d5db;
        color: #9ca3af;
        cursor: not-allowed;
        transform: none;
        box-shadow: none;
    }
    
    /* å›ç­”ã‚¨ãƒªã‚¢ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .ai-response {
        background-color: #f8fafc;
        border: 1px solid #e5e7eb;
        border-radius: 0.75rem;
        padding: 1.5rem;
        margin: 1rem 0;
        font-size: 0.95rem;
        line-height: 1.7;
        color: #374151;
        max-height: 70vh;
        overflow-y: auto;
    }
    
    .ai-response h2, .ai-response h3 {
        color: #1f2937;
        margin-top: 1.5rem;
        margin-bottom: 0.75rem;
    }
    
    .ai-response h2:first-child, .ai-response h3:first-child {
        margin-top: 0;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .css-1d391kg {
        background-color: #f9fafb;
        border-right: 1px solid #e5e7eb;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .css-1d391kg .stButton > button {
        width: 100%;
        font-size: 0.8rem;
        padding: 0.5rem 1rem;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³æ–‡å­—ã‚’ã‚‚ã£ã¨å°ã•ã */
    .css-1d391kg .stCaption {
        font-size: 0.7rem !important;
        color: #6b7280 !important;
        margin-bottom: 0.25rem !important;
        line-height: 1.2 !important;
    }
    
    /* æ–°è¦ãƒãƒ£ãƒƒãƒˆãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .css-1d391kg .stButton > button[title="æ–°è¦ãƒãƒ£ãƒƒãƒˆ"] {
        background-color: #059669 !important;
        color: white !important;
        border: none !important;
        border-radius: 0.375rem !important;
        padding: 0.25rem 0.5rem !important;
        font-size: 0.75rem !important;
        font-weight: 600 !important;
        margin-top: 0.25rem !important;
    }
    
    .css-1d391kg .stButton > button[title="æ–°è¦ãƒãƒ£ãƒƒãƒˆ"]:hover {
        background-color: #047857 !important;
    }
    
    /* ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .streamlit-expanderHeader {
        font-size: 0.875rem;
        font-weight: 500;
        color: #374151;
    }
    
    /* ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    [data-testid="metric-container"] {
        background-color: #ffffff;
        border: 1px solid #e5e7eb;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    }
    
    /* ã‚¢ãƒ©ãƒ¼ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stAlert {
        border-radius: 0.375rem;
        margin-bottom: 0.5rem;
        font-size: 0.75rem;
        border: none;
        padding: 0.5rem 0.75rem;
        font-weight: 500;
    }
    
    .stSuccess {
        background-color: #ecfdf5;
        color: #065f46;
        border-left: 4px solid #10b981;
    }
    
    .stInfo {
        background-color: #eff6ff;
        color: #1e40af;
        border-left: 4px solid #3b82f6;
    }
    
    .stWarning {
        background-color: #fffbeb;
        color: #92400e;
        border-left: 4px solid #f59e0b;
    }
    
    .stError {
        background-color: #fef2f2;
        color: #991b1b;
        border-left: 4px solid #ef4444;
    }
    
    /* ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Šç·š */
    hr {
        border: none;
        border-top: 1px solid #e5e7eb;
        margin: 2rem 0;
    }
    
    /* ã‚¹ãƒ”ãƒŠãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚° */
    .stSpinner {
        text-align: center;
    }
    
    /* ãƒ†ã‚­ã‚¹ãƒˆã®èª¿æ•´ */
    .stMarkdown p {
        margin-bottom: 0.75rem;
        line-height: 1.6;
        color: #374151;
    }
    
    /* å…¥åŠ›è¦ç´ å…¨èˆ¬ã®æ–‡å­—è‰²å¼·åŒ– */
    input, textarea, select {
        color: #111827 !important;
    }
    
    /* Streamlitã®å…¥åŠ›è¦ç´  */
    .stTextInput input {
        color: #111827 !important;
    }
    
    /* å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆ */
    .stCaption {
        font-size: 0.75rem;
        color: #6b7280;
        margin-top: 0.25rem;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¸Šéƒ¨ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ† - ã‚·ãƒ³ãƒ—ãƒ«ã«
    st.markdown('<h1 class="main-title">ğŸ’¼ å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹</h1>', unsafe_allow_html=True)
    st.markdown('<p class="main-subtitle">æ³•äººå‘ã‘ç ”ä¿®äº‹æ¥­ã®å–¶æ¥­æ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ</p>', unsafe_allow_html=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    if 'rag_system' not in st.session_state:
        with st.spinner("ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­..."):
            st.session_state.rag_system = RAGSystem()
    
    rag_system = st.session_state.rag_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ - ChatGPTé¢¨ã®ä¼šè©±å±¥æ­´ã¨ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†
    with st.sidebar:
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º - ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ä¸Šéƒ¨1/5ã«åã‚ã‚‹
        status_container = st.container()
        with status_container:
            # ã‚·ã‚¹ãƒ†ãƒ æº–å‚™çŠ¶æ…‹ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å°ã•ãè¡¨ç¤º
            if hasattr(rag_system, 'embedding_status'):
                if "âŒ" in rag_system.embedding_status:
                    st.caption("âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼")
                else:
                    st.caption("âœ… ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã®ã¿è¡¨ç¤º
            if hasattr(rag_system, 'db_status') and "ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ" in rag_system.db_status:
                import re
                match = re.search(r'(\d+)ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ', rag_system.db_status)
                if match:
                    doc_count = match.group(1)
                    st.caption(f"ğŸ“š {doc_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ©ç”¨å¯èƒ½")
            else:
                st.caption("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­...")
        
        st.markdown("---")
        
        # ä¼šè©±å±¥æ­´
        col_title, col_new = st.columns([2, 1])
        with col_title:
            st.markdown("### ğŸ’¬ ä¼šè©±å±¥æ­´")
        with col_new:
            if st.button("â•", help="æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
                # ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°è¦ãƒãƒ£ãƒƒãƒˆé–‹å§‹
                st.session_state.chat_history = []
                # ãƒ¡ã‚¤ãƒ³å…¥åŠ›ã‚¨ãƒªã‚¢ã‚‚ã‚¯ãƒªã‚¢
                if 'reuse_question' in st.session_state:
                    del st.session_state.reuse_question
                # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å¼·åˆ¶çš„ã«ç©ºã«ã™ã‚‹ãƒ•ãƒ©ã‚°
                st.session_state.clear_input = True
                st.rerun()
        
        # ä¼šè©±å±¥æ­´ã®åˆæœŸåŒ–
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        
        # ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤ºï¼ˆæœ€æ–°5ä»¶ï¼‰
        if st.session_state.chat_history:
            for i, chat in enumerate(st.session_state.chat_history[-5:]):  # æœ€æ–°5ä»¶ã®ã¿è¡¨ç¤º
                with st.container():
                    # ã‚¯ãƒªãƒƒã‚¯ã§è³ªå•ã‚’å†åˆ©ç”¨ã§ãã‚‹ãƒœã‚¿ãƒ³
                    if st.button(
                        f"ğŸ’­ {chat['question'][:30]}..." if len(chat['question']) > 30 else f"ğŸ’­ {chat['question']}",
                        key=f"history_{len(st.session_state.chat_history)-5+i}",
                        use_container_width=True
                    ):
                        # è³ªå•ã‚’å†è¨­å®šï¼ˆå¾Œã§ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã§ä½¿ç”¨ï¼‰
                        st.session_state.reuse_question = chat['question']
                        st.rerun()
        else:
            st.caption("ã¾ã ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # å±¥æ­´ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
        if st.session_state.chat_history:
            if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
                st.session_state.chat_history = []
                st.rerun()
        
        st.markdown("---")
        
        # ä½¿ã„æ–¹ã¨è³ªå•ä¾‹ï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¼ï¼‰
        with st.expander("ğŸ’¡ ä½¿ã„æ–¹"):
            st.markdown("""
            1. ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã€ã§ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
            2. ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«å–¶æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›
            3. AIãŒé–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦å›ç­”
            """)
        
        with st.expander("ğŸ“ è³ªå•ä¾‹"):
            st.markdown("""
            **ä¾¡æ ¼ãƒ»æ–™é‡‘ç³»**
            - æ–°ä»»ç®¡ç†è·ç ”ä¿®ã®æ–™é‡‘ã¯ï¼Ÿ
            - ç ”ä¿®ã®è²»ç”¨å¯¾åŠ¹æœã¯ï¼Ÿ
            
            **é¡§å®¢æƒ…å ±ç³»**
            - ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ§˜ã®èª²é¡Œã¯ï¼Ÿ
            - å¤§æˆå»ºè¨­æ§˜ã¸ã®ææ¡ˆå†…å®¹ã¯ï¼Ÿ
            
            **ç«¶åˆãƒ»å·®åˆ¥åŒ–ç³»**
            - ç«¶åˆä»–ç¤¾ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã¯ï¼Ÿ
            - å»ºè¨­æ¥­ç•Œå‘ã‘ã®ç ”ä¿®å†…å®¹ã¯ï¼Ÿ
            """)
        
        with st.expander("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ è©³ç´°"):
            st.markdown(f"""
            **åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«:** multilingual-e5-small  
            **ãƒ™ã‚¯ãƒˆãƒ«DB:** ChromaDB  
            **LLM:** LM Studio (gpt-oss-20b)  
            **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º:** {CHUNK_SIZE}æ–‡å­—  
            **æ¤œç´¢ç²¾åº¦:** Top-3
            """)
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ - è³ªå•ãƒœãƒƒã‚¯ã‚¹ã‚’ä¸­å¤®ã‚„ã‚„ä¸Šã«é…ç½®
    st.markdown('<div style="margin-top: 3rem;"></div>', unsafe_allow_html=True)
    
    # ä¼šè©±å±¥æ­´ã‹ã‚‰è³ªå•ã‚’å†åˆ©ç”¨ã™ã‚‹å‡¦ç†
    initial_question = ""
    if 'reuse_question' in st.session_state:
        initial_question = st.session_state.reuse_question
        del st.session_state.reuse_question
    
    # æ–°è¦ãƒãƒ£ãƒƒãƒˆãƒœã‚¿ãƒ³ã‹ã‚‰ã®å…¥åŠ›ã‚¯ãƒªã‚¢å‡¦ç†
    form_key_suffix = ""
    if 'clear_input' in st.session_state:
        initial_question = ""  # å¼·åˆ¶çš„ã«ç©ºã«ã™ã‚‹
        # ãƒ•ã‚©ãƒ¼ãƒ ã‚­ãƒ¼ã‚’å¤‰æ›´ã—ã¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å¼·åˆ¶å†åˆæœŸåŒ–
        if 'form_reset_counter' not in st.session_state:
            st.session_state.form_reset_counter = 0
        st.session_state.form_reset_counter += 1
        form_key_suffix = f"_{st.session_state.form_reset_counter}"
        del st.session_state.clear_input
    
    # ãƒ¡ã‚¤ãƒ³è³ªå•å…¥åŠ›ã‚¨ãƒªã‚¢ - Streamlitæ¨™æº–ã§ç¢ºå®Ÿã«å‹•ä½œ
    main_container = st.container()
    with main_container:
        # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form(f"question_form{form_key_suffix}", clear_on_submit=False):
            user_question = st.text_area(
                "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                value=initial_question,
                height=120,
                placeholder="å–¶æ¥­ã«é–¢ã™ã‚‹ã“ã¨ã¯ä½•ã§ã‚‚ãŠç­”ãˆã—ã¾ã™",
                help="ğŸ’¡ ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è³ªå•ã‚’é€ä¿¡ã—ã¦ãã ã•ã„",
                key=f"question_input{form_key_suffix}"
            )
            
            # é€ä¿¡ãƒœã‚¿ãƒ³
            col1, col2, col3 = st.columns([1, 1, 1])
            with col2:
                ask_button = st.form_submit_button(
                    "ğŸ” è³ªå•ã™ã‚‹", 
                    type="primary", 
                    use_container_width=True
                )
    
    # å›ç­”è¡¨ç¤ºã‚¨ãƒªã‚¢ - è³ªå•ãƒœãƒƒã‚¯ã‚¹ã®ç›´ä¸‹
    if ask_button:
        if user_question.strip():
            # å°‘ã—é–“éš”ã‚’ç©ºã‘ã‚‹
            st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", icon="âš ï¸")
    
    if ask_button and user_question.strip():
        
        with st.spinner("ğŸ’­ å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            answer, search_results = rag_system.query(user_question)
            
            # ä¼šè©±å±¥æ­´ã«ä¿å­˜
            import datetime
            chat_entry = {
                "question": user_question,
                "answer": answer,
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
                "sources": [result['source'] for result in search_results] if search_results else []
            }
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆæœ€å¤§20ä»¶ã¾ã§ä¿æŒï¼‰
            if 'chat_history' not in st.session_state:
                st.session_state.chat_history = []
            
            st.session_state.chat_history.append(chat_entry)
            if len(st.session_state.chat_history) > 20:
                st.session_state.chat_history = st.session_state.chat_history[-20:]
            
            # AIå›ç­”ã‚’ç›®ç«‹ã¤ã‚ˆã†ã«è¡¨ç¤º
            st.markdown("### ğŸ¤– AIå›ç­”")
            
            # å›ç­”ã‚’å°‚ç”¨ã‚¹ã‚¿ã‚¤ãƒ«ã§è¡¨ç¤º
            response_container = st.container()
            with response_container:
                st.markdown(f'<div class="ai-response">{answer}</div>', unsafe_allow_html=True)
            
            # å‚è€ƒæƒ…å ±ã¯æŠ˜ã‚ŠãŸãŸã¿å¼ã§è¡¨ç¤º
            if search_results:
                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
                with st.expander("ğŸ“‹ å‚è€ƒã«ã—ãŸæƒ…å ±", expanded=False):
                    for i, result in enumerate(search_results, 1):
                        st.markdown(f"**ğŸ“„ å‚è€ƒæƒ…å ± {i}**")
                        col_a, col_b = st.columns([3, 1])
                        with col_a:
                            st.markdown(f"**å‡ºå…¸:** {result['source']}")
                        with col_b:
                            st.markdown(f"**é¡ä¼¼åº¦:** {1 - result['distance']:.3f}")
                        
                        st.markdown("**å†…å®¹æŠœç²‹:**")
                        content_preview = result['content'][:200] + "..." if len(result['content']) > 200 else result['content']
                        st.markdown(f"_{content_preview}_")
                        
                        if i < len(search_results):
                            st.markdown("---")


if __name__ == "__main__":
    main() 