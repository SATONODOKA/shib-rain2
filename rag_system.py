#!/usr/bin/env python3
"""
å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ RAGã‚·ã‚¹ãƒ†ãƒ  - ç°¡æ½”ç‰ˆ
æ³•äººå‘ã‘ç ”ä¿®äº‹æ¥­ã®å–¶æ¥­æ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
"""

import os
import glob
import chromadb
import streamlit as st
from sentence_transformers import SentenceTransformer
import requests
import datetime
from typing import List, Dict, Any

# è¨­å®š
LM_STUDIO_API_URL = "http://localhost:1234/v1/chat/completions"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
FILE_PATTERNS = ["*.md", "*.txt", "*.docx.md"]

class RAGSystem:
    def __init__(self):
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.setup_embedding_model()
        self.setup_chroma_db()
        self.check_lm_studio_connection()
        self.auto_load_documents()
        
    def setup_embedding_model(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        try:
            self.embedding_model = SentenceTransformer('intfloat/multilingual-e5-small')
            self.embedding_status = "âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†"
            return True
        except Exception as e:
            self.embedding_status = f"âŒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}"
            return False
    
    def setup_chroma_db(self):
        """ChromaDBã®åˆæœŸåŒ–"""
        try:
            self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
            try:
                self.collection = self.chroma_client.get_collection("sales_knowledge")
                self.db_status = f"ğŸ“š æ—¢å­˜ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆ{self.collection.count()}ä»¶ï¼‰"
            except:
                self.collection = self.chroma_client.create_collection(
                    name="sales_knowledge",
                    metadata={"description": "å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"}
                )
                self.db_status = "ğŸ“š æ–°è¦ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ä½œæˆå®Œäº†"
            return True
        except Exception as e:
            self.db_status = f"âŒ ChromaDBåˆæœŸåŒ–å¤±æ•—: {e}"
            return False
    
    def check_lm_studio_connection(self):
        """LM Studioæ¥ç¶šçŠ¶æ³ã®ç¢ºèª"""
        try:
            response = requests.get("http://localhost:1234/v1/models", timeout=5)
            if response.status_code == 200:
                models = response.json()
                if models.get("data"):
                    model_details = []
                    for model in models["data"]:
                        model_id = model.get("id", "unknown")
                        if "gpt-oss" in model_id.lower():
                            model_details.append(f"âœ… {model_id} (æ¨å¥¨)")
                        else:
                            model_details.append(f"ğŸ“‹ {model_id}")
                    
                    self.lm_studio_status = f"âœ… æ¥ç¶šæ¸ˆã¿ - {', '.join(model_details)}"
                else:
                    self.lm_studio_status = "âš ï¸ æ¥ç¶šæ¸ˆã¿ - ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                return True
            else:
                self.lm_studio_status = f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼ - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}"
                return False
        except requests.exceptions.ConnectionError:
            self.lm_studio_status = "âŒ æœªæ¥ç¶š - LM Studioã‚’èµ·å‹•ã—ã¦ãã ã•ã„"
            return False
        except Exception as e:
            self.lm_studio_status = f"âŒ æ¥ç¶šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}"
            return False
    
    def auto_load_documents(self):
        """åˆæœŸåŒ–æ™‚ã«è‡ªå‹•ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            current_count = self.collection.count()
            if current_count > 0:
                self.db_status = f"âœ… {current_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã™"
                return True
            
            documents = self.get_documents()
            if documents:
                self.reset_collection()
                total_chunks = sum(self.process_document(doc_path) for doc_path in documents)
                self.db_status = f"âœ… {len(documents)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{total_chunks}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"
            else:
                self.db_status = "âš ï¸ sample_documentsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        except Exception as e:
            self.db_status = f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
    
    def get_documents(self, document_path: str = "sample_documents") -> List[str]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—"""
        if not os.path.exists(document_path):
            return []
        
        documents = []
        for pattern in FILE_PATTERNS:
            files = glob.glob(os.path.join(document_path, pattern))
            documents.extend(files)
        
        return sorted(documents)
    
    def reset_collection(self):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        try:
            self.chroma_client.delete_collection("sales_knowledge")
        except:
            pass
        self.collection = self.chroma_client.create_collection(
            name="sales_knowledge",
            metadata={"description": "å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"}
        )
    
    def process_document(self, file_path: str) -> int:
        """å€‹åˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                return 0
            
            filename = os.path.basename(file_path)
            if filename.endswith('.md'):
                filename = filename[:-3]
            elif filename.endswith('.txt'):
                filename = filename[:-4]
            
            chunks = self.split_text(content)
            chunks_added = 0
            
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    chunk_id = f"{filename}#chunk-{i+1}"
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    existing = self.collection.query(
                        query_embeddings=[self.embedding_model.encode(chunk).tolist()],
                        n_results=1,
                        where={"chunk_id": chunk_id}
                    )
                    
                    if not existing["documents"][0]:
                        self.collection.add(
                            documents=[chunk],
                            metadatas=[{
                                "source": filename,
                                "chunk_id": chunk_id,
                                "file_path": file_path,
                                "chunk_index": i,
                                "timestamp": datetime.datetime.now().isoformat()
                            }],
                            ids=[f"{filename}_{i}"]
                        )
                        chunks_added += 1
            
            return chunks_added
            
        except Exception as e:
            if 'st' in globals():
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return 0
    
    def split_text(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²"""
        if len(text) <= CHUNK_SIZE:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + CHUNK_SIZE
            
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # æ–‡ã®å¢ƒç•Œã§åˆ†å‰²ã‚’è©¦è¡Œ
            split_point = text.rfind('ã€‚', start, end)
            if split_point > start:
                end = split_point + 1
            
            chunks.append(text[start:end])
            start = end - CHUNK_OVERLAP
            
            if start >= len(text):
                break
        
        return chunks
    
    def search_similar_documents(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢"""
        try:
            if self.collection.count() == 0:
                if 'st' in globals():
                    st.warning("âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™ã€‚")
                return []
            
            query_embedding = self.embedding_model.encode(query).tolist()
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results, self.collection.count()),
                include=["documents", "metadatas", "distances"]
            )
            
            if not results["documents"] or not results["documents"][0]:
                if 'st' in globals():
                    st.warning("âš ï¸ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return []
            
            return [
                {
                    "content": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i],
                    "source": results["metadatas"][0][i]["source"]
                }
                for i in range(len(results["documents"][0]))
            ]
            
        except Exception as e:
            if 'st' in globals():
                st.error(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """LM Studioã‚’ä½¿ç”¨ã—ã¦å›ç­”ç”Ÿæˆ"""
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹
            sorted_docs = sorted(context_docs, key=lambda x: x['distance'])
            context_summary = "\n".join([
                f"{doc['source']}: {doc['content'][:100]}"
                for doc in sorted_docs[:2]
            ])
            
            # è¶…è»½é‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""è³ªå•: {query}
å‚è€ƒ: {context_summary}
å›ç­”:"""

            data = {
                "model": "gpt-oss-20b",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 150,
                "top_p": 0.8,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0,
                "stream": False
            }
            
            response = requests.post(
                LM_STUDIO_API_URL, 
                headers={"Content-Type": "application/json"}, 
                json=data, 
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    if len(content.strip()) < 5:
                        return self._generate_simple_answer(query, context_docs)
                    return content
                else:
                    return "âŒ LM Studioã‹ã‚‰ã®å¿œç­”å½¢å¼ãŒä¸æ­£ã§ã™"
            else:
                return f"âŒ LM Studio APIã‚¨ãƒ©ãƒ¼: {response.status_code}"
                
        except requests.exceptions.ConnectionError:
            return "âŒ LM Studioã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚LM StudioãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        except requests.exceptions.Timeout:
            return "âŒ LM Studioã‹ã‚‰ã®å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚"
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def _generate_simple_answer(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªå›ç­”ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        if not context_docs:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        best_doc = min(context_docs, key=lambda x: x['distance'])
        
        return f"""## ã€å›ç­”ã€‘
è³ªå•: {query}

## ã€é–¢é€£æƒ…å ±ã€‘
{best_doc['source']}ã‚ˆã‚Š:
{best_doc['content'][:300]}...

## ã€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã€‘
ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€å–¶æ¥­æ´»å‹•ã«æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚"""
    
    def query(self, question: str) -> tuple[str, List[Dict[str, Any]]]:
        """RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.check_lm_studio_connection()
        
        search_results = self.search_similar_documents(question, n_results=5)
        
        if not search_results:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", []
        
        # LM Studioæ¥ç¶šãƒã‚§ãƒƒã‚¯ã¨å›ç­”ç”Ÿæˆ
        try:
            response = requests.get("http://localhost:1234/v1/models", timeout=3)
            if response.status_code == 200:
                answer = self.generate_answer(question, search_results)
            else:
                answer = self._generate_simple_answer(question, search_results)
        except:
            answer = self._generate_simple_answer(question, search_results)
        
        return answer, search_results


def get_custom_css():
    """ã‚«ã‚¹ã‚¿ãƒ CSS"""
    return """
    <style>
    .main .block-container {
        padding-top: 0.5rem;
        padding-bottom: 2rem;
        max-width: 50rem;
        margin: 0 auto;
    }
    
    .main-title {
        text-align: left;
        color: #1f2937;
        font-size: 1.5rem;
        font-weight: 700;
        margin-bottom: 0.25rem;
    }
    
    .main-subtitle {
        text-align: left;
        color: #6b7280;
        font-size: 0.8rem;
        margin-bottom: 0.5rem;
    }
    
    .stTextArea textarea {
        border: 1px solid #d1d5db;
        border-radius: 0.75rem;
        padding: 1rem;
        font-size: 0.95rem;
        line-height: 1.5;
        background-color: #ffffff;
        color: #111827 !important;
        resize: none;
        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    }
    
    .stButton > button {
        background-color: #10b981;
        color: white;
        border: none;
        border-radius: 0.5rem;
        padding: 0.625rem 1.5rem;
        font-weight: 600;
        font-size: 0.875rem;
        transition: all 0.2s ease;
    }
    
    .stButton > button:hover {
        background-color: #059669;
        transform: translateY(-1px);
    }
    
    .ai-response {
        background-color: #f8fafc;
        border: 1px solid #e5e7eb;
        border-radius: 0.75rem;
        padding: 1.5rem;
        margin: 1rem 0;
        font-size: 0.95rem;
        line-height: 1.7;
        color: #374151;
        max-height: 70vh;
        overflow-y: auto;
    }
    </style>
    """


def main():
    st.set_page_config(
        page_title="å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ RAGã‚·ã‚¹ãƒ†ãƒ ",
        page_icon="ğŸ’¼",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.markdown(get_custom_css(), unsafe_allow_html=True)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown('<h1 class="main-title">ğŸ’¼ å–¶æ¥­ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹</h1>', unsafe_allow_html=True)
    st.markdown('<p class="main-subtitle">æ³•äººå‘ã‘ç ”ä¿®äº‹æ¥­ã®å–¶æ¥­æ”¯æ´AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ</p>', unsafe_allow_html=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    if 'rag_system' not in st.session_state:
        with st.spinner("ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­..."):
            st.session_state.rag_system = RAGSystem()
    
    rag_system = st.session_state.rag_system
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown("#### âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        
        if hasattr(rag_system, 'embedding_status'):
            status_text = "âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼" if "âŒ" in rag_system.embedding_status else "âœ… ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†"
            st.caption(status_text)
        
        if hasattr(rag_system, 'db_status') and "ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ" in rag_system.db_status:
            import re
            match = re.search(r'(\d+)ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ', rag_system.db_status)
            if match:
                st.caption(f"ğŸ“š {match.group(1)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ©ç”¨å¯èƒ½")
        else:
            st.caption("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­...")
        
        if hasattr(rag_system, 'lm_studio_status'):
            if "âœ…" in rag_system.lm_studio_status:
                st.caption(f"ğŸ¤– {rag_system.lm_studio_status.split(' - ')[1] if ' - ' in rag_system.lm_studio_status else 'LM Studioæ¥ç¶šæ¸ˆã¿'}")
            else:
                st.caption(f"âŒ {rag_system.lm_studio_status}")
        
        if st.button("ğŸ”„ æ¥ç¶šçŠ¶æ³æ›´æ–°", use_container_width=True):
            rag_system.check_lm_studio_connection()
            st.rerun()
        
        st.markdown("---")
        
        # ä¼šè©±å±¥æ­´
        col_title, col_new = st.columns([2, 1])
        with col_title:
            st.markdown("### ğŸ’¬ ä¼šè©±å±¥æ­´")
        with col_new:
            if st.button("â•", help="æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
                st.session_state.chat_history = []
                if 'reuse_question' in st.session_state:
                    del st.session_state.reuse_question
                st.session_state.clear_input = True
                st.rerun()
        
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        
        if st.session_state.chat_history:
            for i, chat in enumerate(st.session_state.chat_history[-5:]):
                display_text = f"ğŸ’­ {chat['question'][:30]}..." if len(chat['question']) > 30 else f"ğŸ’­ {chat['question']}"
                if st.button(display_text, key=f"history_{len(st.session_state.chat_history)-5+i}", use_container_width=True):
                    st.session_state.reuse_question = chat['question']
                    st.rerun()
        else:
            st.caption("ã¾ã ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        
        if st.session_state.chat_history:
            if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
                st.session_state.chat_history = []
                st.rerun()
        
        st.markdown("---")
        
        with st.expander("ğŸ’¡ ä½¿ã„æ–¹"):
            st.markdown("""
            1. ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«å–¶æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›
            2. AIãŒé–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦å›ç­”
            3. ä¼šè©±å±¥æ­´ã‹ã‚‰éå»ã®è³ªå•ã‚’å†åˆ©ç”¨å¯èƒ½
            """)
        
        with st.expander("ğŸ“ è³ªå•ä¾‹"):
            st.markdown("""
            **ä¾¡æ ¼ãƒ»æ–™é‡‘ç³»**
            - æ–°ä»»ç®¡ç†è·ç ”ä¿®ã®æ–™é‡‘ã¯ï¼Ÿ
            - ç ”ä¿®ã®è²»ç”¨å¯¾åŠ¹æœã¯ï¼Ÿ
            
            **é¡§å®¢æƒ…å ±ç³»**
            - ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ§˜ã®èª²é¡Œã¯ï¼Ÿ
            - å¤§æˆå»ºè¨­æ§˜ã¸ã®ææ¡ˆå†…å®¹ã¯ï¼Ÿ
            
            **ç«¶åˆãƒ»å·®åˆ¥åŒ–ç³»**
            - ç«¶åˆä»–ç¤¾ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã¯ï¼Ÿ
            - å»ºè¨­æ¥­ç•Œå‘ã‘ã®ç ”ä¿®å†…å®¹ã¯ï¼Ÿ
            """)
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    st.markdown('<div style="margin-top: 3rem;"></div>', unsafe_allow_html=True)
    
    # å…¥åŠ›å‡¦ç†
    initial_question = ""
    if 'reuse_question' in st.session_state:
        initial_question = st.session_state.reuse_question
        del st.session_state.reuse_question
    
    form_key_suffix = ""
    if 'clear_input' in st.session_state:
        initial_question = ""
        if 'form_reset_counter' not in st.session_state:
            st.session_state.form_reset_counter = 0
        st.session_state.form_reset_counter += 1
        form_key_suffix = f"_{st.session_state.form_reset_counter}"
        del st.session_state.clear_input
    
    # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form(f"question_form{form_key_suffix}", clear_on_submit=False):
        user_question = st.text_area(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=initial_question,
            height=120,
            placeholder="å–¶æ¥­ã«é–¢ã™ã‚‹ã“ã¨ã¯ä½•ã§ã‚‚ãŠç­”ãˆã—ã¾ã™",
            help="ğŸ’¡ ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è³ªå•ã‚’é€ä¿¡ã—ã¦ãã ã•ã„",
            key=f"question_input{form_key_suffix}"
        )
        
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            ask_button = st.form_submit_button(
                "ğŸ” è³ªå•ã™ã‚‹", 
                type="primary", 
                use_container_width=True
            )
    
    # å›ç­”å‡¦ç†
    if ask_button:
        if user_question.strip():
            st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)
            
            with st.spinner("ğŸ’­ å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                answer, search_results = rag_system.query(user_question)
                
                # ä¼šè©±å±¥æ­´ã«ä¿å­˜
                chat_entry = {
                    "question": user_question,
                    "answer": answer,
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "sources": [result['source'] for result in search_results] if search_results else []
                }
                
                st.session_state.chat_history.append(chat_entry)
                if len(st.session_state.chat_history) > 20:
                    st.session_state.chat_history = st.session_state.chat_history[-20:]
                
                # AIå›ç­”è¡¨ç¤º
                st.markdown("### ğŸ¤– AIå›ç­”")
                st.markdown(f'<div class="ai-response">{answer}</div>', unsafe_allow_html=True)
                
                # å‚è€ƒæƒ…å ±
                if search_results:
                    st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
                    with st.expander("ğŸ“‹ å‚è€ƒã«ã—ãŸæƒ…å ±", expanded=False):
                        for i, result in enumerate(search_results, 1):
                            st.markdown(f"**ğŸ“„ å‚è€ƒæƒ…å ± {i}**")
                            col_a, col_b = st.columns([3, 1])
                            with col_a:
                                st.markdown(f"**å‡ºå…¸:** {result['source']}")
                            with col_b:
                                st.markdown(f"**é¡ä¼¼åº¦:** {1 - result['distance']:.3f}")
                            
                            st.markdown("**å†…å®¹æŠœç²‹:**")
                            content_preview = result['content'][:200] + "..." if len(result['content']) > 200 else result['content']
                            st.markdown(f"_{content_preview}_")
                            
                            if i < len(search_results):
                                st.markdown("---")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", icon="âš ï¸")


if __name__ == "__main__":
    main() 